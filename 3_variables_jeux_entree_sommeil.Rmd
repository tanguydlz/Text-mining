---
title: "Text_mining"
author: "Cong Bang Huynh"
date: "12/26/2020"
output: word_document
---



```{r}
library(openxlsx)
```

```{r}
df=read.xlsx('C:/Users/DELL/Desktop/Master SISE/Text Mining/BDD_CEDA_dec_2020_complete - ANONYME.xlsx')
```


```{r}
library(tm)
```

```{r}
#source
activite_source=VectorSource(df$jeux.et.activités)
```







```{r}
#corpus

activite_corpus=VCorpus(activite_source)

```


```{r}
#minuscule 
activite_corpus= tm_map(activite_corpus,content_transformer(tolower))
activite_corpus = tm_map(activite_corpus,removePunctuation)
activite_corpus = tm_map(activite_corpus,removeWords, stopwords('fr'))
activite_corpus = tm_map(activite_corpus,stripWhitespace)
```


```{r}
BigramTokenizer =function(x){
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
```





```{r}
activite_matrix=DocumentTermMatrix(activite_corpus, control = list(tokenize=BigramTokenizer))
```


```{r}

#matrice
M1 <- as.matrix(activite_matrix)
print(nrow(M1))
print(ncol(M1))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq <- apply(M1,2,sum)
print(sort(freq,decreasing=TRUE)[1:20])
```



```{r}
findAssocs(activite_matrix, "jouer", 0.1)

```










################################################################################################



```{r}
#source
s_source=VectorSource(df$Sommeil)
```

```{r}
#corpus

s_corpus=VCorpus(s_source)

```


```{r}
#minuscule 
s_corpus= tm_map(s_corpus,content_transformer(tolower))
s_corpus = tm_map(s_corpus,removePunctuation)
s_corpus = tm_map(s_corpus,removeWords, stopwords('fr'))
s_corpus = tm_map(s_corpus,stripWhitespace)
```

```{r}
library(ngram)

```



```{r}

#tokenizer <- function(x){
#  ngram_asweka(x, min = 2, max = 2)
#}

#tokenizer <- function(x) {
#  NGramTokenizer(x, Weka_control(min = 2, max = 2))
#}

```



```{r}
BigramTokenizer =function(x){
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
```




```{r}
s_matrix=DocumentTermMatrix(s_corpus, control = list(tokenize=BigramTokenizer))
```


```{r}

#matrice
M1 <- as.matrix(s_matrix)
print(nrow(M1))
print(ncol(M1))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq <- apply(M1,2,sum)
fre=c(83,freq)
names(fre)[1]='Non'
print(sort(fre,decreasing=TRUE)[1:10])
```
```{r}
library(wordcloud)
```



```{r}
wordcloud(names(fre),fre,max.words=25,color="blue")
```

```{r}
findAssocs(s_matrix,"difficultés dendormissement",0.2)
```




```{r}
#d1 <- dist(M1,method="euclidian")
#cah.ward <- hclust(d1,method="ward.D2")
#plot(cah.ward)
```

```{r}

##multidimensional scaling
mds <- cmdscale(d1,eig=T,k=2)
print(mds)

#proportion de variance expliquée
prop.var <- cumsum(mds$eig)/sum(mds$eig) * 100
print(prop.var)

#nuage de points dans le plan factoriel
plot(mds$points[,1],mds$points[,2],type="n")
text(mds$points[,1],mds$points[,2],labels=1:7)

```











#################################################




```{r}
#source
s_source=VectorSource(df$Entrée.en.contact)
```

```{r}
#corpus

s_corpus=VCorpus(s_source)

```


```{r}
#minuscule 
s_corpus= tm_map(s_corpus,content_transformer(tolower))
s_corpus = tm_map(s_corpus,removePunctuation)
s_corpus = tm_map(s_corpus,removeWords, stopwords('fr'))
s_corpus = tm_map(s_corpus,stripWhitespace)
```

```{r}
library(ngram)

```



```{r}

#tokenizer <- function(x){
#  ngram_asweka(x, min = 2, max = 2)
#}

#tokenizer <- function(x) {
#  NGramTokenizer(x, Weka_control(min = 2, max = 2))
#}

```



```{r}
BigramTokenizer =function(x){
    unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
}
```




```{r}
s_matrix=TermDocumentMatrix(s_corpus, control = list(tokenize=BigramTokenizer))
```


```{r}

#matrice
M1 <- as.matrix(s_matrix)
print(nrow(M1))
print(ncol(M1))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq <- apply(M1,1,sum)

print(sort(freq,decreasing=TRUE)[1:10])
```


```{r}
library(wordcloud)
```



```{r}
wordcloud(names(fre),fre,max.words=25,color="blue")
```

```{r}
findAssocs(s_matrix,"difficultés dendormissement",0.2)
```




































