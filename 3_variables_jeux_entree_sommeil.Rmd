---
title: "Text_mining"
author: "Cong Bang Huynh"
date: "12/26/2020"
output: word_document
---



```{r}
library(openxlsx)
```

```{r}
df=read.xlsx('C:/Users/DELL/Desktop/Master SISE/Text Mining/BDD_CEDA_dec_2020_complete - ANONYME.xlsx')
```


```{r}
library(tm)
```

```{r}
#source
activite_source=VectorSource(df$jeux.et.activités)
```







```{r}
#corpus

activite_corpus=VCorpus(activite_source)

```


```{r}
#minuscule 
activite_corpus= tm_map(activite_corpus,content_transformer(tolower))
activite_corpus = tm_map(activite_corpus,removePunctuation)
activite_corpus = tm_map(activite_corpus,removeWords, stopwords('fr'))
activite_corpus = tm_map(activite_corpus,stripWhitespace)
```


```{r}
BigramTokenizer =function(x){
    unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
}
```





```{r}
activite_matrix=DocumentTermMatrix(activite_corpus, control = list(tokenize=BigramTokenizer))
```


```{r}

#matrice
M1 <- as.matrix(activite_matrix)
print(nrow(M1))
print(ncol(M1))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq <- apply(M1,2,sum)
print(sort(freq,decreasing=TRUE)[1:20])
```



```{r}
findAssocs(activite_matrix, "jouer", 0.1)

```







```{r}
library(topicmodels)
```

```{r}
ind=c()
for (i in 1:183){
  if (sum(M1[i,])!=0){
    ind=c(ind,i)
  }
}
```


```{r}
ap_lda <- LDA(activite_matrix[ind,], k = 2)#, control = list(seed = 1234))
ap_lda
```


```{r}
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
```




```{r}
library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```




```{r}
ap_documents <- tidy(ap_lda, matrix = "gamma")
ap_documents
```



```{r}
head(df,20)
```





```{r}
ap_top_terms <- ap_documents %>%
  group_by(topic) %>%
  top_n(50, gamma) %>%
  ungroup() %>%
  arrange(topic, -gamma)

ap_top_terms %>%
  mutate(term = reorder_within(document, gamma, topic)) %>%
  ggplot(aes(gamma, document, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```












################################################################################################



```{r}
#source
s_source=VectorSource(df$Sommeil)
```


```{r}
#corpus

s_corpus=VCorpus(s_source)

```


```{r}
#minuscule 
s_corpus= tm_map(s_corpus,content_transformer(tolower))
s_corpus = tm_map(s_corpus,removePunctuation)
s_corpus = tm_map(s_corpus,removeWords, stopwords('fr'))
s_corpus = tm_map(s_corpus,stripWhitespace)
```

```{r}
library(ngram)

```



```{r}

#tokenizer <- function(x){
#  ngram_asweka(x, min = 2, max = 2)
#}

#tokenizer <- function(x) {
#  NGramTokenizer(x, Weka_control(min = 2, max = 2))
#}

```



```{r}
BigramTokenizer =function(x){
    unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)
}
```




```{r}
s_matrix=DocumentTermMatrix(s_corpus, control = list(tokenize=BigramTokenizer))
```

```{r}

#matrice
M1 <- as.matrix(s_matrix)
print(nrow(M1))
print(ncol(M1))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq <- apply(M1,2,sum)
fre=c(83,freq)
names(fre)[1]='Non'
print(sort(fre,decreasing=TRUE)[1:10])
```
```{r}
library(wordcloud)
```



```{r}
wordcloud(names(fre),fre,max.words=25,color="blue")
```

```{r}
#réalisation d'une AFC
library(FactoMineR)
afc <- CA(M1,ncp=2,graph=FALSE)

```




```{r}
#graphique en rajoutant du bruit pour éviter les chevauchements
plot(afc$col$coord[,1],afc$col$coord[,2],type="n")
abline(h=0,v=0)
text(afc$col$coord[,1],jitter(afc$col$coord[,2],500),labels=rownames(afc$col$coord),col="red",cex=0.75)
text(afc$row$coord[,1],afc$row$coord[,2],labels=rownames(afc$row$coord),col="blue")
```




```{r}
library(topicmodels)
```

```{r}
ind=c()
for (i in 1:183){
  if (sum(M1[i,])!=0){
    ind=c(ind,i)
  }
}
```


```{r}
ap_lda <- LDA(s_matrix[ind,], k = 2)#, control = list(seed = 1234))
ap_lda
```


```{r}
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
```




```{r}
library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r}
ap_documents <- tidy(ap_lda, matrix = "gamma")
ap_documents
```


```{r}
ap_top_terms <- ap_documents %>%
  group_by(topic) %>%
  top_n(20, gamma) %>%
  ungroup() %>%
  arrange(topic, -gamma)

ap_top_terms %>%
  mutate(term = reorder_within(document, gamma, topic)) %>%
  ggplot(aes(gamma, document, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```































```{r}
findAssocs(s_matrix,"difficultés dendormissement",0.2)
```




```{r}
#d1 <- dist(M1,method="euclidian")
#cah.ward <- hclust(d1,method="ward.D2")
#plot(cah.ward)
```

```{r}

##multidimensional scaling
mds <- cmdscale(d1,eig=T,k=2)
print(mds)

#proportion de variance expliquée
prop.var <- cumsum(mds$eig)/sum(mds$eig) * 100
print(prop.var)

#nuage de points dans le plan factoriel
plot(mds$points[,1],mds$points[,2],type="n")
text(mds$points[,1],mds$points[,2],labels=1:7)

```











#################################################




```{r}
#source
s_source=VectorSource(df$Entrée.en.contact)
```

```{r}
#corpus

s_corpus=VCorpus(s_source)

```


```{r}
#minuscule 
s_corpus= tm_map(s_corpus,content_transformer(tolower))
s_corpus = tm_map(s_corpus,removePunctuation)
s_corpus = tm_map(s_corpus,removeWords, stopwords('fr'))
s_corpus = tm_map(s_corpus,stripWhitespace)
```

```{r}
library(ngram)

```



```{r}

#tokenizer <- function(x){
#  ngram_asweka(x, min = 2, max = 2)
#}

#tokenizer <- function(x) {
#  NGramTokenizer(x, Weka_control(min = 2, max = 2))
#}

```



```{r}
BigramTokenizer =function(x){
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
```




```{r}
s_matrix=TermDocumentMatrix(s_corpus, control = list(tokenize=BigramTokenizer))
```


```{r}

#matrice
M1 <- as.matrix(s_matrix)
print(nrow(M1))
print(ncol(M1))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq <- apply(M1,1,sum)

print(sort(freq,decreasing=TRUE)[1:10])
```


```{r}
library(wordcloud)
```



```{r}
wordcloud(names(fre),fre,max.words=25,color="blue")
```

```{r}
findAssocs(s_matrix,"difficultés dendormissement",0.2)
```








```{r}
library(topicmodels)
```

```{r}
ind=c()
for (i in 1:183){
  if (sum(M1[i,])!=0){
    ind=c(ind,i)
  }
}
```


```{r}
ap_lda <- LDA(s_matrix[ind,], k = 2)#, control = list(seed = 1234))
ap_lda
```


```{r}
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
```




```{r}
library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```





























