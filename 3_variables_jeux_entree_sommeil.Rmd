---
title: "Text_mining"
author: "Cong Bang Huynh"
date: "12/26/2020"
output: word_document
---

## IMPORTER DES DONNEES

```{r}
library(openxlsx)
library("readxl")
library(tm)
library(ngram)
library(wordcloud)
```

```{r}


#df=read.xlsx('C:/Users/DELL/Desktop/Master SISE/Text Mining/BDD_CEDA_dec_2020_complete - ANONYME.xlsx')

setwd("/Users/hoangkhanhle/Desktop/School/Master 2/Text Mining/Projet")
df <- read_excel("Base_CEDA_nov_2020_anonymisee.xlsx")
```



## ------------- Function BigramTokenizer ---------------

```{r}
BigramTokenizer =function(x){
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
```
################################################################################################
################################################################################################
##------------------------------------- JEUX ET ACTIVITE ---------------------------------------
################################################################################################
################################################################################################

```{r}
#source
activite_source=VectorSource(df$`jeux et activités`)
```


```{r}
#corpus

activite_corpus=VCorpus(activite_source)

```


```{r}
#minuscule 
activite_corpus= tm_map(activite_corpus,content_transformer(tolower))
activite_corpus = tm_map(activite_corpus,removePunctuation)
activite_corpus = tm_map(activite_corpus,removeWords, stopwords('fr'))
activite_corpus = tm_map(activite_corpus,stripWhitespace)
```

```{r}
BigramTokenizer =function(x){
    unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)
}
```





```{r}
activite_matrix=DocumentTermMatrix(activite_corpus, control = list(tokenize=BigramTokenizer))
```


```{r}

#matrice
M1 <- as.matrix(activite_matrix)
print(nrow(M1))
print(ncol(M1))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq <- apply(M1,2,sum)
print(sort(freq,decreasing=TRUE)[1:20])
```


```{r}
findAssocs(activite_matrix, "jouer", 0.1)

```







```{r}
library(topicmodels)
```

```{r}
ind=c()
for (i in 1:183){
  if (sum(M1[i,])!=0){
    ind=c(ind,i)
  }
}
```


```{r}
ap_lda <- LDA(activite_matrix[ind,], k = 2)#, control = list(seed = 1234))
ap_lda
```


```{r}
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
```




```{r}
library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```




```{r}
ap_documents <- tidy(ap_lda, matrix = "gamma")
ap_documents
```



```{r}
head(df,20)
```





```{r}
ap_top_terms <- ap_documents %>%
  group_by(topic) %>%
  top_n(50, gamma) %>%
  ungroup() %>%
  arrange(topic, -gamma)

ap_top_terms %>%
  mutate(term = reorder_within(document, gamma, topic)) %>%
  ggplot(aes(gamma, document, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```












################################################################################################
################################################################################################
## ------------------------------------- SOMMEIL -----------------------------------------------
################################################################################################
################################################################################################

```{r}
#source
s_source=VectorSource(df$Sommeil)
```


```{r}
#corpus

s_corpus=VCorpus(s_source)

```


```{r}
#minuscule 
s_corpus= tm_map(s_corpus,content_transformer(tolower))
s_corpus = tm_map(s_corpus,removePunctuation)
s_corpus = tm_map(s_corpus,removeWords, stopwords('fr'))
s_corpus = tm_map(s_corpus,stripWhitespace)
```

```{r}

#tokenizer <- function(x){
#  ngram_asweka(x, min = 2, max = 2)
#}

#tokenizer <- function(x) {
#  NGramTokenizer(x, Weka_control(min = 2, max = 2))
#}

```



```{r}
BigramTokenizer =function(x){
    unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)
}
```




```{r}
s_matrix=DocumentTermMatrix(s_corpus, control = list(tokenize=BigramTokenizer))
```

```{r}

#matrice
M1 <- as.matrix(s_matrix)
print(nrow(M1))
print(ncol(M1))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq <- apply(M1,2,sum)
fre=c(83,freq)
names(fre)[1]='Non'
print(sort(fre,decreasing=TRUE)[1:10])
```



```{r}
wordcloud(names(fre),fre,max.words=25,color="blue")
```

```{r}
#réalisation d'une AFC
library(FactoMineR)
afc <- CA(M1,ncp=2,graph=FALSE)

```




```{r}
#graphique en rajoutant du bruit pour éviter les chevauchements
plot(afc$col$coord[,1],afc$col$coord[,2],type="n")
abline(h=0,v=0)
text(afc$col$coord[,1],jitter(afc$col$coord[,2],500),labels=rownames(afc$col$coord),col="red",cex=0.75)
text(afc$row$coord[,1],afc$row$coord[,2],labels=rownames(afc$row$coord),col="blue")
```




```{r}
library(topicmodels)
```

```{r}
ind=c()
for (i in 1:183){
  if (sum(M1[i,])!=0){
    ind=c(ind,i)
  }
}
```


```{r}
ap_lda <- LDA(s_matrix[ind,], k = 2)#, control = list(seed = 1234))
ap_lda
```


```{r}
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
```




```{r}
library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```{r}
ap_documents <- tidy(ap_lda, matrix = "gamma")
ap_documents
```


```{r}
ap_top_terms <- ap_documents %>%
  group_by(topic) %>%
  top_n(20, gamma) %>%
  ungroup() %>%
  arrange(topic, -gamma)

ap_top_terms %>%
  mutate(term = reorder_within(document, gamma, topic)) %>%
  ggplot(aes(gamma, document, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```































```{r}
findAssocs(s_matrix,"difficultés dendormissement",0.2)
```


```{r}
#d1 <- dist(M1,method="euclidian")
#cah.ward <- hclust(d1,method="ward.D2")
#plot(cah.ward)
```

```{r}

##multidimensional scaling
mds <- cmdscale(d1,eig=T,k=2)
print(mds)

#proportion de variance expliquée
prop.var <- cumsum(mds$eig)/sum(mds$eig) * 100
print(prop.var)

#nuage de points dans le plan factoriel
plot(mds$points[,1],mds$points[,2],type="n")
text(mds$points[,1],mds$points[,2],labels=1:7)

```





################################################################################################
################################################################################################
## ------------------------------- ENTREE EN CONTACT ------------------------------------------
################################################################################################
################################################################################################




```{r}
#source
#s_source=VectorSource(df$Entrée.en.contact)
s_source=VectorSource(df$`Entrée en contact`)
```

```{r}
#corpus

s_corpus=VCorpus(s_source)

```


```{r}
#minuscule 
s_corpus= tm_map(s_corpus,content_transformer(tolower))
s_corpus = tm_map(s_corpus,removePunctuation)
s_corpus = tm_map(s_corpus,removeWords, stopwords('fr'))
s_corpus = tm_map(s_corpus,stripWhitespace)
```


```{r}

#tokenizer <- function(x){
#  ngram_asweka(x, min = 2, max = 2)
#}

#tokenizer <- function(x) {
#  NGramTokenizer(x, Weka_control(min = 2, max = 2))
#}

```



```{r}
BigramTokenizer =function(x){
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
```




```{r}
s_matrix=TermDocumentMatrix(s_corpus, control = list(tokenize=BigramTokenizer))
```


```{r}

#matrice
M1 <- as.matrix(s_matrix)
print(nrow(M1))
print(ncol(M1))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq <- apply(M1,1,sum)

print(sort(freq,decreasing=TRUE)[1:10])
```


```{r}
wordcloud(names(freq),freq,max.words=25,color="blue")
```

```{r}
findAssocs(s_matrix,"difficultés dendormissement",0.2)
```





################################################################################################
################################################################################################
## ----------------------------- HABITUDE DE L'ENFANT  ------------------------------------------
################################################################################################
################################################################################################


```{r}
#source
he_source=VectorSource(df$`Habitudes de l'enfant`)
```


```{r}
#corpus
he_corpus=VCorpus(he_source)

```


```{r}
#TransformMinuscule 
he_corpus= tm_map(he_corpus,content_transformer(tolower))
#removePunctuation
he_corpus = tm_map(he_corpus,removePunctuation)
#removeStopWord
he_corpus = tm_map(he_corpus,removeWords, stopwords('fr'))
#removeWhiteSpace
he_corpus = tm_map(he_corpus,stripWhitespace)
```


```{r}
he_matrix=TermDocumentMatrix(he_corpus, control = list(tokenize=BigramTokenizer))
```



```{r}

```{r}
library(topicmodels)
```

```{r}
ind=c()
for (i in 1:183){
  if (sum(M1[i,])!=0){
    ind=c(ind,i)
  }
}
```

print(sort(freq_he,decreasing=TRUE)[1:10])
```

```{r}
ap_lda <- LDA(s_matrix[ind,], k = 2)#, control = list(seed = 1234))
ap_lda
```


```{r}
library(tidytext)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics
```



################################################################################################
################################################################################################
## ----------------------------- QUI  ------------------------------------------
################################################################################################
################################################################################################

```{r}
library(ggplot2)
library(dplyr)

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

```


```{r}
#TransformMinuscule 
q_corpus= tm_map(q_corpus,content_transformer(tolower))
#removePunctuation
q_corpus = tm_map(q_corpus,removePunctuation)
#removeStopWord
q_corpus = tm_map(q_corpus,removeWords, stopwords('fr'))
#removeWhiteSpace
q_corpus = tm_map(q_corpus,stripWhitespace)
```

```{r}
q_matrix=TermDocumentMatrix(q_corpus, control = list(tokenize=BigramTokenizer))
```



```{r}

#matrice
M_q <- as.matrix(q_matrix)
print(nrow(M_q))
print(ncol(M_q))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq_q <- apply(M_q,1,sum)

print(sort(freq_q,decreasing=TRUE)[1:10])
```
```{r}
wordcloud(names(freq_q),freq_q,max.words=25,color="blue")
```



## ----------------- Vue Global de database ------------------------------

```{r}
class(df)
str(df)
```

################################################################################################
################################################################################################
## ----------------------------- RAISON  ------------------------------------------
################################################################################################
################################################################################################

Valeur manquant 
```{r}
#Var Raison sans données manquantes : 161 obs
df$Raison[df$Raison!=""]
length(df$Raison[df$Raison!=""])
```













