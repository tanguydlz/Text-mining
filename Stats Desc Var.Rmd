---
title: "Text_mining"
author: "Cong Bang Huynh"
date: "12/26/2020"
output: word_document
---

## --------------- CONG BANG ---------------------- ########

## IMPORTER DES DONNEES

```{r}
library(openxlsx)
library("readxl")
library(tm)
library(ngram)
library(wordcloud)
```

```{r}


#df=read.xlsx('C:/Users/DELL/Desktop/Master SISE/Text Mining/BDD_CEDA_dec_2020_complete - ANONYME.xlsx')

setwd("/Users/hoangkhanhle/Desktop/School/Master 2/Text Mining/Projet")
library(xlsx)
df<-read.xlsx2("BDD_CEDA_dec_2020_complete - ANONYME.xlsx", sheetIndex = 1,header = TRUE)
```



## ------------- Function BigramTokenizer ---------------

```{r}
BigramTokenizer =function(x){
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
}
```

## ------------- Function OnegramTokenizer ---------------

```{r}
OnegramTokenizer =function(x){
    unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = FALSE)
}
```

################################################################################################
################################################################################################
##------------------------------------- JEUX ET ACTIVITE ---------------------------------------
################################################################################################
################################################################################################

```{r}
#source
activite_source=VectorSource(df$`jeux et activités`)
```


```{r}
#corpus

activite_corpus=VCorpus(activite_source)

```


```{r}
#minuscule 
activite_corpus= tm_map(activite_corpus,content_transformer(tolower))
activite_corpus = tm_map(activite_corpus,removePunctuation)
activite_corpus = tm_map(activite_corpus,removeWords, stopwords('fr'))
activite_corpus = tm_map(activite_corpus,stripWhitespace)
```

```{r}
activite_matrix=DocumentTermMatrix(activite_corpus, control = list(tokenize=BigramTokenizer))
```


```{r}

#matrice
M1 <- as.matrix(activite_matrix)
print(nrow(M1))
print(ncol(M1))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq <- apply(M1,2,sum)
print(sort(freq,decreasing=TRUE)[1:20])
```


```{r}
findAssocs(activite_matrix, "jouer", 0.1)

```





################################################################################################
################################################################################################
## ------------------------------------- SOMMEIL -----------------------------------------------
################################################################################################
################################################################################################

```{r}
#source
s_source=VectorSource(df$Sommeil)
```

```{r}
#corpus

s_corpus=VCorpus(s_source)

```


```{r}
#minuscule 
s_corpus= tm_map(s_corpus,content_transformer(tolower))
s_corpus = tm_map(s_corpus,removePunctuation)
s_corpus = tm_map(s_corpus,removeWords, stopwords('fr'))
s_corpus = tm_map(s_corpus,stripWhitespace)
```

```{r}

#tokenizer <- function(x){
#  ngram_asweka(x, min = 2, max = 2)
#}

#tokenizer <- function(x) {
#  NGramTokenizer(x, Weka_control(min = 2, max = 2))
#}

```



```{r}
s_matrix=DocumentTermMatrix(s_corpus, control = list(tokenize=BigramTokenizer))
```


```{r}

#matrice
M1 <- as.matrix(s_matrix)
print(nrow(M1))
print(ncol(M1))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq <- apply(M1,2,sum)
fre=c(83,freq)
names(fre)[1]='Non'
print(sort(fre,decreasing=TRUE)[1:10])
```



```{r}
wordcloud(names(fre),fre,max.words=25,color="blue")
```

```{r}
findAssocs(s_matrix,"difficultés dendormissement",0.2)
```


```{r}
#d1 <- dist(M1,method="euclidian")
#cah.ward <- hclust(d1,method="ward.D2")
#plot(cah.ward)
```

```{r}

##multidimensional scaling
mds <- cmdscale(d1,eig=T,k=2)
print(mds)

#proportion de variance expliquée
prop.var <- cumsum(mds$eig)/sum(mds$eig) * 100
print(prop.var)

#nuage de points dans le plan factoriel
plot(mds$points[,1],mds$points[,2],type="n")
text(mds$points[,1],mds$points[,2],labels=1:7)

```





################################################################################################
################################################################################################
## ------------------------------- ENTREE EN CONTACT ------------------------------------------
################################################################################################
################################################################################################




```{r}
#source
#s_source=VectorSource(df$Entrée.en.contact)
s_source=VectorSource(df$`Entrée en contact`)
```

```{r}
#corpus

s_corpus=VCorpus(s_source)

```


```{r}
#minuscule 
s_corpus= tm_map(s_corpus,content_transformer(tolower))
s_corpus = tm_map(s_corpus,removePunctuation)
s_corpus = tm_map(s_corpus,removeWords, stopwords('fr'))
s_corpus = tm_map(s_corpus,stripWhitespace)
```


```{r}
s_matrix=TermDocumentMatrix(s_corpus, control = list(tokenize=BigramTokenizer))
```


```{r}

#matrice
M1 <- as.matrix(s_matrix)
print(nrow(M1))
print(ncol(M1))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq <- apply(M1,1,sum)

print(sort(freq,decreasing=TRUE)[1:10])
```


```{r}
wordcloud(names(freq),freq,max.words=25,color="blue")
```

```{r}
findAssocs(s_matrix,"difficultés dendormissement",0.2)
```



## -------------------------- KHANH --------------------------------- ##



################################################################################################
################################################################################################
## ----------------------------- HABITUDE DE L'ENFANT  ------------------------------------------
################################################################################################
################################################################################################


```{r}
#source
he_source=VectorSource(df$`Habitudes de l'enfant`)
```


```{r}
#corpus
he_corpus=VCorpus(he_source)

```


```{r}
#TransformMinuscule 
he_corpus= tm_map(he_corpus,content_transformer(tolower))
#removePunctuation
he_corpus = tm_map(he_corpus,removePunctuation)
#removeStopWord
he_corpus = tm_map(he_corpus,removeWords, stopwords('fr'))
#removeWhiteSpace
he_corpus = tm_map(he_corpus,stripWhitespace)
```


```{r}
he_matrix=TermDocumentMatrix(he_corpus, control = list(tokenize=BigramTokenizer))
```



```{r}

#matrice
M_he <- as.matrix(he_matrix)
print(nrow(M_he))
print(ncol(M_he))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq_he <- apply(M_he,1,sum)

print(sort(freq_he,decreasing=TRUE)[1:10])
```

```{r}
library(wordcloud)
```



```{r}
wordcloud(names(freq_he),freq_he,max.words=25,color="blue")
```



################################################################################################
################################################################################################
## ----------------------------- QUI  ------------------------------------------
################################################################################################
################################################################################################

```{r}
#source
q_source=VectorSource(df$Qui)
```


```{r}
#corpus
q_corpus=VCorpus(q_source)

```


```{r}
#TransformMinuscule 
q_corpus= tm_map(q_corpus,content_transformer(tolower))
#removePunctuation
q_corpus = tm_map(q_corpus,removePunctuation)
#removeStopWord
q_corpus = tm_map(q_corpus,removeWords, stopwords('fr'))
#removeWhiteSpace
q_corpus = tm_map(q_corpus,stripWhitespace)
```

```{r}
q_matrix=TermDocumentMatrix(q_corpus, control = list(tokenize=BigramTokenizer))
```



```{r}

#matrice
M_q <- as.matrix(q_matrix)
print(nrow(M_q))
print(ncol(M_q))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq_q <- apply(M_q,1,sum)

print(sort(freq_q,decreasing=TRUE)[1:10])
```
```{r}
wordcloud(names(freq_q),freq_q,max.words=25,color="blue")
```


## -------------------------- TANGUY --------------------------------- ##



################################################################################################
################################################################################################
## ----------------------------- Preoccupation  ------------------------------------------
################################################################################################
################################################################################################


```{r}
#source
preoccupation_source=VectorSource(df$Préoccupations)
```


```{r}
#corpus
preoccupation_corpus=VCorpus(preoccupation_source)

```


```{r}
#TransformMinuscule 
preoccupation_corpus= tm_map(preoccupation_corpus,content_transformer(tolower))
#removePunctuation
preoccupation_corpus = tm_map(preoccupation_corpus,removePunctuation)
#removeStopWord
preoccupation_corpus = tm_map(preoccupation_corpus,removeWords, stopwords('fr'))
#removeWhiteSpace
preoccupation_corpus = tm_map(preoccupation_corpus,stripWhitespace)
```


```{r}
preoccupation_matrix=TermDocumentMatrix(preoccupation_corpus, control = list(tokenize=BigramTokenizer))
```



```{r}

#matrice
M_preoccupation <- as.matrix(preoccupation_matrix)
print(nrow(M_preoccupation))
print(ncol(M_preoccupation))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq_preoccupation <- apply(M_preoccupation,1,sum)

print(sort(freq_preoccupation,decreasing=TRUE)[1:10])
```


```{r}
wordcloud(names(freq_preoccupation),freq_preoccupation,max.words=20,color=c("lightblue3","lightblue4","darkcyan"))
```

################################################################################################
################################################################################################
## ----------------------------- Antécédents  ------------------------------------------
################################################################################################
################################################################################################


```{r}
library(dplyr)
#source
X=df[,21]
X = recode(X, '0' = "non")
X[X == ""] = "NC"
antecedent_source=VectorSource(X)
```


```{r}
#corpus
antecedent_corpus=VCorpus(antecedent_source)

```


```{r}
#TransformMinuscule 
antecedent_corpus= tm_map(antecedent_corpus,content_transformer(tolower))
#removePunctuation
antecedent_corpus = tm_map(antecedent_corpus,removePunctuation)
#removeStopWord
antecedent_corpus = tm_map(antecedent_corpus,removeWords, stopwords('fr'))
#removeWhiteSpace
antecedent_corpus = tm_map(antecedent_corpus,stripWhitespace)
```


```{r}
antecedent_matrix=TermDocumentMatrix(antecedent_corpus, control = list(tokenize=BigramTokenizer))
```



```{r}

#matrice
M_antecedent <- as.matrix(antecedent_matrix)
print(nrow(M_antecedent))
print(ncol(M_antecedent))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq_antecedent <- apply(M_antecedent,1,sum)

print(sort(freq_antecedent,decreasing=TRUE)[1:10])
```


```{r}
wordcloud(names(freq_antecedent),freq_antecedent,max.words=25,color="blue")
```



## --------------------AMELIE  -----------------------------------------------


## ----------------- Vue Global de database ------------------------------

```{r}
class(df)
str(df)
```

################################################################################################
################################################################################################
## ----------------------------- RAISON  ------------------------------------------
################################################################################################
################################################################################################



Valeur manquante : 

```{r}
#Var Raison sans données manquantes : 177 obs
VarRaison<-df$Raison[df$Raison!=""]
length(VarRaison)
```

Analyse sur 177 individus :

```{r}
#source
raison_source=VectorSource(VarRaison)
```


```{r}
#corpus
raison_corpus=VCorpus(raison_source)

```


```{r}
#TransformMinuscule 
raison_corpus= tm_map(raison_corpus,content_transformer(tolower))
#removePunctuation
raison_corpus = tm_map(raison_corpus,removePunctuation)
#removeStopWord
raison_corpus = tm_map(raison_corpus,removeWords, stopwords('fr'))
#removeWhiteSpace
raison_corpus = tm_map(raison_corpus,stripWhitespace)
```

Matrice contenant les termes deux à deux : 
```{r}
raison_matrix=TermDocumentMatrix(raison_corpus, control = list(tokenize=BigramTokenizer))
```


```{r}

#matrice
M_raison <- as.matrix(raison_matrix)
print(nrow(M_raison))
print(ncol(M_raison))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq_raison <- apply(M_raison,1,sum)

print(sort(freq_raison,decreasing=TRUE)[1:10])
```


```{r}
wordcloud(names(freq_raison),freq_raison,max.words=20,color=c("lightblue3","lightblue4","darkcyan"))
```


Matrice contenant terme par terme : 
```{r}
raison_matrix2=TermDocumentMatrix(raison_corpus, control = list(tokenize=OnegramTokenizer))
```



```{r}

#matrice
M_raison2 <- as.matrix(raison_matrix2)
print(nrow(M_raison2))
print(ncol(M_raison2))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq_raison2 <- apply(M_raison2,1,sum)

print(sort(freq_raison2,decreasing=TRUE)[1:10])
```


```{r}
wordcloud(names(freq_raison2),freq_raison2,max.words=20,color=c("lightblue3","lightblue4","darkcyan"))
```
Figure : Nuage de mots de la variable "Raison"

Ce nuage de est plus intéressant que celui comparant deux à deux les termes.

Les mots qui ressortent le plus dans cette variable sont "comportement", "language", "retard" "enfants" et "autre", sur un échantillon de 177 individus.

################################################################################################
################################################################################################
## ----------------------------- Première Inquiétude  ------------------------------------------
################################################################################################
################################################################################################

Recodage 1er inquiétude en mois :
Nous remplaçons les données en chaine de caratcére, par des nombres pour réaliser une analyse. Pour cela nous remplaçons les chaines contenant le terme "naissance" par 0 pour 0 mois, puis nous multiplions par 12 les nombres précédant le terme "ans". Et pour finir nous transformons la variable en type numéric.

```{r}
library(stringr)
#Des la naissance : 0
df$X1eres.inquiétudes..mois.[str_detect(df$X1eres.inquiétudes..mois.,"naissance")]<-0
#Recodage années en mois :
for (i in 1:length(df$X1eres.inquiétudes..mois.)) {
  if (str_detect(df$X1eres.inquiétudes..mois.[i],"ans")) {
    df$X1eres.inquiétudes..mois.[i]<-as.numeric(substr(df$X1eres.inquiétudes..mois.[i],1,str_locate(df$X1eres.inquiétudes..mois.[i], " ans")[1]-1))*12
  }
}
#transforme la variable en numéric pour garder que les valeurs numéric
df$X1eres.inquiétudes..mois.<-as.numeric(df$X1eres.inquiétudes..mois.)

```


Comme cette variable est quantitatives, nous allons voir une table des statistiques descriptives, ainsi qu'un boxplot pour la représentation graphique :

```{r}
summary(df$X1eres.inquiétudes..mois.)
```
Table : table statistiques sur la variable quantitative "première inquiétude"

La variable première inquiétude posséde 23 valeurs manquante, donc nous analysons un échantillon de 163 individus. 
En moyenne c'est à l'age de 31,5 mois que les parents ont leur première inquiétude, et la médiane est de 24 mois. Ce qui signifie que quelques parents ont eu des inquiétudes plus tard et tire à augmenter la moyenne. Nous pouvons remarquer dans notre échantillon que l'age de l'enfant le plus grand est de 168 mois donc 14 ans.

```{r}
library(ggplot2)
ggplot(data = df, aes(x = "", y = X1eres.inquiétudes..mois.)) + 
  geom_boxplot() +
  ggtitle("Boxplot de l'âge des enfants lors de la première inquiétude des parents") +
  ylab("Première Inquiétude en mois")
```

Figure : Boxplot de la variable première inquiétude en mois

Sur un échantillon de 163 individus, nous pouvons voir qu'il existe plusieurs données extremes Mais que la médianes et bien au alentour de 24 mois. 


################################################################################################
################################################################################################
## ----------------------------- Deuxiéme partie d'analyse à l'aide du diagnostique  ------------------------------------------
################################################################################################
################################################################################################

Sujet 2 : "Quels inquiétudes et descriptions cliniques concordent avec un diagnostic avéré d’autisme après évaluation complète sur notre centre.

Nous allons prendre en compte la variable diagnostic dans cette deuxième analyse :

```{r}
VarDiag<-df$Diagnostic.[df$Diagnostic.!=""]
length(VarDiag)
```
La variable diagnostic posséde 3 valeurs manquantes. Donc nous enleverons ces individus de la suite de l'analyse. Notre nouvel échantillon posséde 183 individus.

Premièrement, nous allons voir les statistiques descriptives de cette variable.

Analyse sur 183 individus :

```{r}
#source
diag_source=VectorSource(VarDiag)
```


```{r}
#corpus
diag_corpus=VCorpus(diag_source)

```


```{r}
#TransformMinuscule 
diag_corpus= tm_map(diag_corpus,content_transformer(tolower))
#removePunctuation
diag_corpus = tm_map(diag_corpus,removePunctuation)
#removeStopWord
diag_corpus = tm_map(diag_corpus,removeWords, stopwords('fr'))
#removeWhiteSpace
diag_corpus = tm_map(diag_corpus,stripWhitespace)
```

Matrice contenant les termes deux à deux : 
```{r}
diag_matrix=TermDocumentMatrix(diag_corpus, control = list(tokenize=BigramTokenizer))
```


```{r}

#matrice
M_diag <- as.matrix(diag_matrix)
print(nrow(M_diag))
print(ncol(M_diag))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq_diag <- apply(M_diag,1,sum)

print(sort(freq_diag,decreasing=TRUE)[1:10])
```


```{r}
wordcloud(names(freq_diag),freq_diag,max.words=20,color=c("lightblue3","lightblue4","darkcyan"))
```

Les pairs de mots qui ressortent le plus dans cette variable sont "tsa retard", "retard language", "trouble language" "tsa tdah" et "tsa di", sur un échantillon de 183 individus.


Matrice contenant terme par terme : 
```{r}
diag_matrix2=TermDocumentMatrix(diag_corpus, control = list(tokenize=OnegramTokenizer))
```



```{r}

#matrice
M_diag2 <- as.matrix(diag_matrix2)
print(nrow(M_diag2))
print(ncol(M_diag2))

#liste des termes
#print(colnames(M1))
#fréquence de chaque terme
freq_diag2 <- apply(M_diag2,1,sum)

print(sort(freq_diag2,decreasing=TRUE)[1:10])
```


```{r}
wordcloud(names(freq_diag2),freq_diag2,max.words=20,color=c("lightblue3","lightblue4","darkcyan"))
```
Figure : Nuage de mots de la variable "diagnostic"

Ce nuage de est moins intéressant que celui comparant deux à deux les termes.


QUESTION : 
Comment reconnait-on un diagnostique d'autisme avérer grâce à cette variable ?

Uniquement ceux qui sont diagnostiqué "TSA" ?



